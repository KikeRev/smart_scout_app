{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c8a21-839f-4b28-8393-48c988a995c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import hashlib\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from newspaper import Article\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sqlalchemy import orm\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from bs4 import BeautifulSoup\n",
    "import requests, re\n",
    "import torch\n",
    "from transformers import logging as hf_logging\n",
    "import feedparser\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# fuera de las funciones, para que se cargue una vez\n",
    "_SUMMARIZER = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",   # o t5-small / pegasus\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "# tokenizer para contar tokens (opcional, si quieres trocear artículos muy largos)\n",
    "_TOKENIZER = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "MAX_TOKENS = 1024 \n",
    "\n",
    "EMB_MODEL = \"sentence-transformers/all-mpnet-base-v2\"  # 768 d\n",
    "embedder = SentenceTransformer(EMB_MODEL)\n",
    "\n",
    "EMB_DIM = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94cca6-68d3-4236-b6ef-cd36ce141586",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEEDS: List[Tuple[str, str]] = [\n",
    "    (\"as_la_liga\", \"https://feeds.as.com/mrss-s/pages/as/site/as.com/section/futbol/subsection/primera/\"),\n",
    "    (\"as_la_liga_hypermotion\", \"https://feeds.as.com/mrss-s/pages/as/site/as.com/section/futbol/subsection/segunda/\"),\n",
    "    (\"as_champions_league\", \"https://feeds.as.com/mrss-s/pages/as/site/as.com/section/futbol/subsection/champions/\"),\n",
    "    (\"marca_primera_division\", \"https://e00-marca.uecdn.es/rss/futbol/primera-division.xml\"),\n",
    "    (\"marca_segunda_division\", \"https://e00-marca.uecdn.es/rss/futbol/segunda-division.xml\"),\n",
    "    (\"marca_champions_league\", \"https://e00-marca.uecdn.es/rss/futbol/champions-league.xml\"),\n",
    "    (\"marca_premier_league\", \"https://e00-marca.uecdn.es/rss/futbol/premier-league.xml\"),\n",
    "    (\"marca_bundesliga\", \"https://e00-marca.uecdn.es/rss/futbol/bundesliga.xml\"),\n",
    "    (\"marca_seria_a\", \"https://e00-marca.uecdn.es/rss/futbol/liga-italiana.xml\"),\n",
    "    (\"marca_ligue_1\", \"https://e00-marca.uecdn.es/rss/futbol/liga-francesa.xml\"),\n",
    "    (\"marca_america\", \"https://e00-marca.uecdn.es/rss/futbol/america.xml\"),\n",
    "    (\"transfermarkt_es\",\"https://www.transfermarkt.es/rss/news\"),\n",
    "    (\"transfermarkt_uk\",\"https://www.transfermarkt.co.uk/rss/news\"),\n",
    "    (\"transfermarkt_it\",\"https://www.transfermarkt.it/rss/news\"),\n",
    "    (\"transfermarkt_de\",\"https://www.transfermarkt.de/rss/news\"),\n",
    "    (\"transfermarkt_pt\",\"https://www.transfermarkt.pt/rss/news\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b751acf-f508-4f0f-99d7-9e7b5df2be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_rss_items() -> List[dict]:\n",
    "    \"\"\"Return list of dicts with keys: source, title, url, published_at (UTC).\"\"\"\n",
    "\n",
    "    items: List[dict] = []\n",
    "    now = datetime.now(tz=timezone.utc)\n",
    "\n",
    "    for source_id, feed_url in FEEDS:\n",
    "        try:\n",
    "            parsed = feedparser.parse(feed_url)\n",
    "        except Exception as exc:\n",
    "            print(f\"[feed-error] {source_id}: {exc}\")\n",
    "            continue\n",
    "\n",
    "        for entry in parsed.entries:\n",
    "            # Robust date handling ------------------------------------------------\n",
    "            if hasattr(entry, \"published_parsed\") and entry.published_parsed:\n",
    "                published_at = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)\n",
    "            elif hasattr(entry, \"updated_parsed\") and entry.updated_parsed:\n",
    "                published_at = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)\n",
    "            else:\n",
    "                published_at = now\n",
    "\n",
    "            items.append(\n",
    "                {\n",
    "                    \"source\": source_id,\n",
    "                    \"title\": entry.title,\n",
    "                    \"url\": entry.link,\n",
    "                    \"published_at\": published_at,\n",
    "                }\n",
    "            )\n",
    "    return items\n",
    "\n",
    "\n",
    "# ----------------- Article parsing & embeddings ------------------\n",
    "\n",
    "def safe_summarize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Resume un texto con ajuste automático de longitudes y\n",
    "    fallback si el modelo falla.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # tokens reales del chunk\n",
    "        n_tokens = len(_TOKENIZER(text).input_ids)\n",
    "\n",
    "        # Queremos algo más corto que el original pero > min_length\n",
    "        max_len = max(20, int(n_tokens * 0.8))    # 80 % del tamaño\n",
    "        max_len = min(max_len, 128)               # nunca > 128\n",
    "        min_len = max(10, int(max_len * 0.25))    # 25 % del max_len\n",
    "\n",
    "        return _SUMMARIZER(\n",
    "            text,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            do_sample=False,\n",
    "        )[0][\"summary_text\"]\n",
    "\n",
    "    except Exception:\n",
    "        # fallback: primeros 400 caracteres\n",
    "        return text[:400] + \"…\"\n",
    "\n",
    "def parse_article(url: str) -> tuple[str, str] | None:\n",
    "    try:\n",
    "        html = requests.get(url, timeout=10).text\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    text = re.sub(r\"\\s+\", \" \", soup.get_text(\" \", strip=True))\n",
    "\n",
    "    if len(text.split()) < 20:\n",
    "        return None\n",
    "\n",
    "    # Split by tokens ≤1024 para BART\n",
    "    tokens = _TOKENIZER(text).input_ids\n",
    "    chunks = []\n",
    "    while tokens:\n",
    "        chunk_ids, tokens = tokens[:1024], tokens[1024:]\n",
    "        chunks.append(_TOKENIZER.decode(chunk_ids, skip_special_tokens=True))\n",
    "\n",
    "    # Resumen jerárquico\n",
    "    summaries = [safe_summarize(c) for c in chunks]\n",
    "    full_summary = safe_summarize(\" \".join(summaries))\n",
    "    return text, full_summary\n",
    "\n",
    "\n",
    "def embed_texts(texts: list[str]) -> list[list[float]]:\n",
    "    # Filtra nulos y vacíos\n",
    "    valid_texts = [t for t in texts if t]\n",
    "    if not valid_texts:\n",
    "        return []\n",
    "\n",
    "    return embedder.encode(\n",
    "        valid_texts,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).tolist()\n",
    "\n",
    "\n",
    "def ingest_news(engine: sa.Engine):\n",
    "    items = sorted(fetch_rss_items(), key=lambda x: x[\"published_at\"], reverse=True)\n",
    "    print(f\"Fetched {len(items)} RSS items → processing …\")\n",
    "\n",
    "    texts:      list[str]  = []   # artículo completo\n",
    "    summaries:  list[str]  = []   # resumen\n",
    "    metas:      list[dict] = []   # metadatos URL, título, fecha…\n",
    "\n",
    "    for meta in items:\n",
    "        try:\n",
    "            print(f\"Parsing article: {meta['url']}\")\n",
    "            parsed = parse_article(meta[\"url\"])\n",
    "\n",
    "            # ── descarta los que no devuelven nada ────────────────────────\n",
    "            if parsed is None:\n",
    "                continue\n",
    "\n",
    "            text, summary = parsed\n",
    "            texts.append(text)\n",
    "            summaries.append(summary)\n",
    "            metas.append(meta)\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[article-error] {meta['url']}: {exc}\")\n",
    "\n",
    "    if not summaries:\n",
    "        print(\"No articles parsed, skipping embeddings.\")\n",
    "        return\n",
    "\n",
    "    # Usa RESÚMENES (o texts) para la embedding; los dos tienen la misma len\n",
    "    embeddings = embed_texts(texts)\n",
    "\n",
    "    with orm.Session(engine) as session:\n",
    "        inserted = 0\n",
    "        for text, summary, emb, meta in zip(texts, summaries, embeddings, metas):\n",
    "            if session.query(FootballNews).filter_by(url=meta[\"url\"]).first():\n",
    "                continue  # duplicado\n",
    "\n",
    "            session.add(\n",
    "                FootballNews(\n",
    "                    url         = meta[\"url\"],\n",
    "                    title       = meta[\"title\"],\n",
    "                    published_at= meta[\"published_at\"],\n",
    "                    article_text= text,\n",
    "                    summary     = summary,\n",
    "                    embedding   = list(map(float, emb)),\n",
    "                    article_meta= {\"source\": meta[\"source\"]},\n",
    "                )\n",
    "            )\n",
    "            inserted += 1\n",
    "        session.commit()\n",
    "\n",
    "    print(f\"✅ News upserted: {inserted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78299819-2e2b-408f-acfa-b7d3446e850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "items =fetch_rss_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52c2fd-4814-433f-912b-3d5ff5021ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2e738-da8a-4c06-b876-d1f0a58e2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sorted(items, key=lambda x: x[\"published_at\"], reverse=True)[:1]\n",
    "print(f\"Fetched {len(items)} RSS items → processing …\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba9fde-e477-40d0-9b11-2d6311c84225",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries: List[str] = []\n",
    "metas: List[dict] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4b7b0-fc84-4d28-bc09-b6617b8be112",
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta in items:\n",
    "    try:\n",
    "        print(f\"Parsing article: {meta['url']}\")\n",
    "        parsed = parse_article(meta[\"url\"])\n",
    "        if parsed is None:\n",
    "            continue\n",
    "        text, summary = parsed\n",
    "        summaries.append(summary)\n",
    "        metas.append(meta)\n",
    "    except Exception as exc:\n",
    "        print(f\"[article-error] {meta['url']}: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817e870-6cad-46f2-83ac-055687f20351",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512082e-20ee-49a0-bd77-06e5f765cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca649d92-bc1e-4192-a1b5-0a62001842d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
